[vocalTract]

# VTL specific parameters
lib = vtl-2.1               # name of used library
speaker = JD2               # name of speaker file
speakertype = male          # type of speaker
audiosamplingrate = 22050   # sampling rate of audio generated by VTL


[ambientSpeech]

# which sounds to babble, can be either vowels or syllables
# vowels are one-character words, syllables multiple-character words, @ refers to the SCHWA sound, syllables have to be defined as a VTL gesture in goalspeech/data/kroeger_ges/
sequences = a e i o u @

# define which of the sequences to use as a pre-known neutral sound
neutral = @

# amount of noise to add to each articulatory parameter
arnoise = 0.1 0.1 0.1 0.1 0.1 0.1 0.01 0.01 0.01 0.01 0.01 0.01 0.01 0.01 0.01 0.01 0.01 0.01

# duration of sounds (only required for vowels)
duration = 500

# how many samples for each defined sound should be used in the ambient speech set (the real amount might be smaller as invalid sounds are filtered out)
numsamplespersequence = 100

# which acoustic features to use. Options include formants (mean formants over the whole speech signal), formants_full (computes formants for multiple frames), mfcc, mfcc_formants, gbfb...
featuremode = formants_full

# whether to preprocess the acoustic features using the model space representation (recommended for features which include dynamics)
usemodelspace = 1


[dynamics]

# parameters for DMP representation of the articulatory trajectories
dmpbfs = 7
dmpk = 0.1
dmpsigma = 0.15
dmptau = 1

# parameters for the model space
esndim = 10                 # in not defined, default 10 is used
esnreg = 1                  # if not defined, default 1 is used
# the seed used for determining the random weights for the ESN reservoir
esnseed = 5                 # if not defined, default 1 is used


[goalSpace]

# dimensionality of target goal space, embedding method
targetdim = 2
embmethod = lda

# if > 0, perform a PCA prior to "embmethod" to this dimension
ldapreprojectiondim = 10

# 'original' means that the original PCA/LDA transforms are used for mapping new data into the goal space (alternatively, a learned mapping could be used)
learnermethod = original
# 'fake' means that target distributions are automatically fitted to ambient speech data
targetlocmethod = fake


[invmodel]
# inverse model parameters
class = OnlineWeightedRBF   # inverse model type
lrate = 0.9                 # learning rate
radius = 0.15               # radius of clusters in goal space
softmax = 1                 # whether to use softmax function for the output

# weighting schemes to be used. Can be e.g. 'tar' (target), 'sal' (saliency of vowels) or 'syl' (syllable pattern similarity).
wschemes = tar syl

# the minimum weight for including newly babbled sounds in the inverse model
weightthreshold = 0.1


[wsm]

# work space model which tracks the progress, used for automatic determination of articulatory noise and for selecting clusters for exploration actively
tskthreshold = 0.1          # radius of the clusters
wthrnodecreation = 0.5      # weight threshold for generating a node in the WSM


[experiment]

runs = 10                   # how many runs to perform of the experiment
maxiterations = 500         # maximum iteration number
evaluationInterval = 10     # frequency of evaluation
# error threshold for stopping before 500 iterations
stopthreshold = 0.1


[babbling]

rollouts = 10               # number of sounds produced in each iteration
tsknoiseamplitude = 0.05    # noise added in task space to explore around the current goal
actnoiseamplitude = 0.55    # amplitude of noise in action space (if fixed)
actnoiseauto = False        # whether to fix exploration noise in action space or not
actnoisefactor = 0.55       # maximum threshold of action noise (if adaptive)
active = False              # whether to select goals actively based on competence (True) or randomly (False)


[wschemeParams]

# parameter for the TargetWeighting scheme: maximum acceptable distance in task space
distancethreshold = 1.5


[locations]

# defines where to find/store data, configuration, results and gesture files for syllables
datadir = data/ambientSpeech
configdir = config
resultsdir = data/results
gesturefiledir = data/kroeger_ges

# for syllables only
gesturefiledir = data/kroeger_ges                   # where to find gestures files
octavescripts = libs/reference-feature-extraction   # location of octave scripts to include

